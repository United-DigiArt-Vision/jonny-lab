# Daily Learning Sweep â€” 2026-02-14

**Gescannt:** 128 neue Artikel (521 unread total), 25 Feeds

---

## Top Findings

### 1. ğŸ”¥ OpenAI killt GPT-4o + Legacy Models (HEUTE!)
**Quelle:** CNET, TechCrunch
- Ab heute 10am PT: GPT-5, GPT-4o, GPT-4.1, GPT-4.1 mini, o4-mini werden deprecated
- Gleichzeitig: Ads auf Free + Go Plans eingefÃ¼hrt
- Community trauert um GPT-4o (Sycophancy-Favorit, besonders bei AI Companions)
- **Relevanz fÃ¼r uns:** BestÃ¤tigt Trend zu Model-Konsolidierung. Wer auf spezifische Modelle setzt = Risiko. Unsere Multi-Model-Strategie (Opus als Default, flexibel) ist richtig.
- **Monetarisierung:** "AI Model Migration Service" â€” Unternehmen helfen die an 4o-Workflows hÃ¤ngen

### 2. ğŸ§  Thoughtworks: Future of Software Engineering Retreat
**Quelle:** Simon Willison / Thoughtworks Report
- Juniors sind PROFITABLER als je zuvor dank AI (kommen schneller aus net-negative Phase)
- Juniors sind BESSER mit AI Tools als Seniors (keine alten Habits)
- ECHTES PROBLEM: Mid-Level Engineers aus dem Hiring-Boom â€” zu viele, schwer umzutrainieren
- Niemand hat bisher eine LÃ¶sung fÃ¼r Mid-Level Retraining
- **Relevanz fÃ¼r uns:** BestÃ¤tigt unsere Positionierung. Wir kÃ¶nnen Mid-Level Engineers helfen, AI zu adoptieren. Training/Consulting-Angebot.

### 3. ğŸ“š Anthropic + CodePath Partnership
**Quelle:** Anthropic Blog
- Claude + Claude Code werden ins CS-Curriculum von 20.000+ Studenten integriert
- Fokus auf Community Colleges, HBCUs (>40% aus Familien <$50k/Jahr)
- Howard University bietet bereits AI-Kurs mit Claude fÃ¼r Credits an
- **Relevanz fÃ¼r uns:** Anthropic investiert massiv in Developer-Education. Mehr Claude-fÃ¤hige Devs = grÃ¶ÃŸerer Markt fÃ¼r unsere Claude-basierten Services.

### 4. ğŸ”Š SoproTTS v1.5 â€” 135M Parameter Voice Cloning
**Quelle:** r/MachineLearning
- Zero-Shot Voice Cloning mit nur 135M Parametern
- Trainiert fÃ¼r ~$100 auf 1 GPU
- LÃ¤uft 20x Realtime auf CPU
- **Relevanz fÃ¼r uns:** Extrem leichtgewichtiges TTS. KÃ¶nnte ElevenLabs fÃ¼r manche Use Cases ersetzen. Lokal, kostenlos, schnell. Evaluieren fÃ¼r unsere TTS-Pipeline.

### 5. ğŸ“Š Research: Higher Effort = Lower Accuracy bei Deep Research
**Quelle:** r/MachineLearning
- GPT-5 und Gemini Flash 3 Deep Research: hÃ¶here Effort-Settings reduzieren Accuracy
- Counterintuitive â€” mehr Compute â‰  bessere Ergebnisse bei Research-Tasks
- **Relevanz fÃ¼r uns:** BestÃ¤tigt dass wir bei Research-Tasks nicht blind "max effort" setzen sollten. Optimale Settings finden.

---

## Nicht relevant (Ã¼bersprungen)
- Reddit r/hiring, r/freelance_forhire: 99% Art/Design Commissions, keine Dev-Jobs
- Verge: Beats Sale, Steam beta, Presidents Day deals
- TechCrunch: Fisker SEC, Tenga hack, Trump Mobile â€” irrelevant
